{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming with Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = spark.readStream.format('kafka'). \\\n",
    "    option('kafka.bootstrap.servers', 'master:9092,slave01:9092,slave02:9092,slave03:9092,slave04:9092,slave05:9092'). \\\n",
    "    option('subscribe', 'gios'). \\\n",
    "    option('startingOffsets', 'earliest'). \\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = d.selectExpr('CAST(key AS STRING)', 'CAST(value AS STRING)', 'partition', 'offset', 'timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "qWithValueAsDouble = q.withColumn('value_as_double', q['value'].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Kafka's timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "qWithTs = qWithValueAsDouble.withColumn('day', qWithValueAsDouble['timestamp'].substr(1, 10)). \\\n",
    "            withColumn('parsed_timestamp', F.to_timestamp('timestamp', 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qWithDD = qWithTs.withColumn('day_diff', F.datediff(F.current_timestamp(), qWithTs['parsed_timestamp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering (getting last 5 _full_ days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = qWithDD.filter((qWithDD['day_diff'] > 0) & (qWithDD['day_diff'] < 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by station name and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered.groupBy('key', 'day').avg('value_as_double').withColumnRenamed('avg(value_as_double)', 'avg_NO2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5de63c8048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.writeStream.format('memory').queryName('in_memory').outputMode('complete').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+------------------+\n",
      "|rank|                 key|       day|           avg_NO2|\n",
      "+----+--------------------+----------+------------------+\n",
      "|   1| KMŚ Puszcza Borecka|2020-05-19|0.5323618181818182|\n",
      "|   2| KMŚ Puszcza Borecka|2020-05-18|0.4100247826086957|\n",
      "|   1|WIOŚ Biskupiec-Mo...|2020-05-20|25.828500000000002|\n",
      "|   2|WIOŚ Biskupiec-Mo...|2020-05-22| 25.45395217391304|\n",
      "|   1|WIOŚ Elbląg ul. B...|2020-05-19| 9.562459565217392|\n",
      "|   2|WIOŚ Elbląg ul. B...|2020-05-18| 7.971984347826088|\n",
      "|   1|            WIOŚ Ełk|2020-05-19| 6.003737826086956|\n",
      "|   2|            WIOŚ Ełk|2020-05-22| 4.208602173913043|\n",
      "|   1|WIOŚ Gołdap ul. J...|2020-05-19|  6.43403695652174|\n",
      "|   2|WIOŚ Gołdap ul. J...|2020-05-22| 5.944795652173913|\n",
      "|   1|WIOŚ Olsztyn ul. ...|2020-05-18| 5.636427826086957|\n",
      "|   2|WIOŚ Olsztyn ul. ...|2020-05-19| 4.982760434782608|\n",
      "|   1|WIOŚ Ostróda Piłs...|2020-05-18| 7.895448695652173|\n",
      "|   2|WIOŚ Ostróda Piłs...|2020-05-22| 7.210130434782608|\n",
      "+----+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT t.rank, t.key, t.day, t.avg_NO2 FROM (SELECT key, day, avg_NO2, DENSE_RANK() OVER (PARTITION BY key ORDER BY avg_NO2 DESC) as rank FROM in_memory) as t WHERE t.rank < 3 ORDER BY t.key ASC, t.avg_NO2 DESC').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[IPyKernel] PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
